import openai # OpenAI's official Python library
import logging
# Updated import path for LlmServiceBase
from src.e_Infra.g_McpInfra.b_LlmManager.LlmServiceBase import LlmServiceBase

logger = logging.getLogger(__name__)

class OpenAIService(LlmServiceBase):
    """
    A service class to encapsulate interactions with the OpenAI API,
    adhering to the LlmServiceBase interface.
    """
    DEFAULT_MODEL_NAME = "gpt-3.5-turbo"    # As per new defaults
    DEFAULT_TEMPERATURE = 0.2              # As per new defaults
    DEFAULT_MAX_OUTPUT_TOKENS = 2000       # As per new defaults

    def __init__(self, api_key: str, model_name: str = None, temperature: float = None, max_output_tokens: int = None):
        """
        Initializes the OpenAIService.

        Args:
            api_key (str): The OpenAI API key.
            model_name (str, optional): Specific OpenAI model. Defaults to self.DEFAULT_MODEL_NAME.
            temperature (float, optional): Sampling temperature. Defaults to self.DEFAULT_TEMPERATURE.
            max_output_tokens (int, optional): Max tokens for response. Defaults to self.DEFAULT_MAX_OUTPUT_TOKENS.
        Raises:
            ValueError: If API key is not provided or params are invalid.
            RuntimeError: If OpenAI client initialization fails.
        """
        if not api_key:
            logger.error("API key not provided for OpenAIService initialization.")
            raise ValueError("API key for OpenAI must be provided.")

        self.api_key = api_key
        self.model_name = model_name or self.DEFAULT_MODEL_NAME

        if temperature is not None:
            try:
                self.temperature = float(temperature)
                if not (0.0 <= self.temperature <= 2.0): # OpenAI typical range
                     logger.warning(f"Temperature {self.temperature} for OpenAI is outside typical range (0.0-2.0). Using it anyway.")
            except ValueError:
                logger.error(f"Invalid temperature value '{temperature}'. Must be a float. Using default {self.DEFAULT_TEMPERATURE}.")
                self.temperature = self.DEFAULT_TEMPERATURE
        else:
            self.temperature = self.DEFAULT_TEMPERATURE

        if max_output_tokens is not None:
            try:
                self.max_output_tokens = int(max_output_tokens)
                if self.max_output_tokens <= 0:
                    logger.warning(f"Invalid max_output_tokens {self.max_output_tokens}. Must be positive. Using default {self.DEFAULT_MAX_OUTPUT_TOKENS}.")
                    self.max_output_tokens = self.DEFAULT_MAX_OUTPUT_TOKENS
            except ValueError:
                logger.error(f"Invalid max_output_tokens value '{max_output_tokens}'. Must be an int. Using default {self.DEFAULT_MAX_OUTPUT_TOKENS}.")
                self.max_output_tokens = self.DEFAULT_MAX_OUTPUT_TOKENS
        else:
            self.max_output_tokens = self.DEFAULT_MAX_OUTPUT_TOKENS

        try:
            self.client = openai.OpenAI(api_key=self.api_key)
            logger.info(f"OpenAIService initialized: model='{self.model_name}', temperature={self.temperature}, max_output_tokens={self.max_output_tokens}.")
        except Exception as e:
            logger.error(f"Error during OpenAIService init (model: {self.model_name}): {e}", exc_info=True)
            raise RuntimeError(f"Failed to initialize OpenAI Service (model: {self.model_name}): {e}")

    def generate_text(self, prompt: str) -> str:
        """
        Generates text using the configured OpenAI model.

        Args:
            prompt (str): The prompt to send to the OpenAI API.
                          This will be used as the content of a user message in the chat completion.

        Returns:
            str: The text response generated by the OpenAI API.

        Raises:
            RuntimeError: If there's an error during text generation.
        """
        if not prompt:
            logger.warning("Generate_text called with an empty prompt.")
            return ""

        try:
            chat_completion = self.client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
                model=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_output_tokens
            )
            if chat_completion.choices and chat_completion.choices[0].message and chat_completion.choices[0].message.content:
                return chat_completion.choices[0].message.content.strip()
            else:
                logger.error("OpenAI API returned an unexpected response structure or empty content.")
                raise RuntimeError("OpenAI API returned no content.")
        except openai.APIError as e:
            logger.error(f"OpenAI API error during text generation (model: {self.model_name}): {e}", exc_info=True)
            raise RuntimeError(f"Failed to generate text using OpenAI (model: {self.model_name}): {e}")
        except Exception as e:
            logger.error(f"Unexpected error during OpenAI text generation (model: {self.model_name}): {e}", exc_info=True)
            raise RuntimeError(f"An unexpected error occurred with OpenAI (model: {self.model_name}): {e}")

    def check_connection(self) -> bool:
        """
        Performs a lightweight check to see if the configured API key
        is valid and can connect to the OpenAI service by listing available models.
        """
        try:
            self.client.models.list()
            logger.info("OpenAI API connection check successful: Models listed.")
            return True
        except openai.APIError as e:
            logger.error(f"OpenAI API connection check failed: {e}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"Unexpected error during OpenAI connection check: {e}", exc_info=True)
            return False
