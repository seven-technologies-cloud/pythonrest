import google.generativeai as genai
import logging

# Configure logging for this module
logger = logging.getLogger(__name__)

# Assuming LlmServiceBase is in a sibling directory 'j_LlmManager'
# Adjust import path if the directory structure is different or if using a central 'interfaces' package
from src.e_Infra.j_LlmManager.LlmServiceBase import LlmServiceBase


class GeminiService(LlmServiceBase):
    """
    A service class to encapsulate interactions with the Google Gemini API,
    adhering to the LlmServiceBase interface.
    """
    DEFAULT_MODEL_NAME = 'gemini-pro'
    DEFAULT_TEMPERATURE = 0.7 # A common default temperature

    def __init__(self, api_key: str, model_name: str = None, temperature: float = None):
        """
        Initializes the GeminiService.

        Args:
            api_key (str): The Google Gemini API key.
            model_name (str, optional): The specific Gemini model name to use. Defaults to 'gemini-pro'.
            temperature (float, optional): Sampling temperature. Defaults to self.DEFAULT_TEMPERATURE.
        Raises:
            ValueError: If the API key is not provided or temperature is invalid.
            RuntimeError: If configuration of the genai client fails.
        """
        if not api_key:
            logger.error("API key not provided for GeminiService initialization.")
            raise ValueError("API key for Google Gemini must be provided.")

        self.api_key = api_key
        self.model_name = model_name or self.DEFAULT_MODEL_NAME

        if temperature is not None:
            try:
                self.temperature = float(temperature)
                if not (0.0 <= self.temperature <= 1.0): # Gemini typical range, can be up to 2.0 for some models
                     logger.warning(f"Temperature {self.temperature} for Gemini is outside typical effective range (0.0-1.0). Using it anyway.")
            except ValueError:
                logger.error(f"Invalid temperature value '{temperature}'. Must be a float. Using default.")
                self.temperature = self.DEFAULT_TEMPERATURE
        else:
            self.temperature = self.DEFAULT_TEMPERATURE

        self.generation_config = genai.types.GenerationConfig(
            temperature=self.temperature
            # Add other generation parameters here if needed, e.g., top_p, top_k, max_output_tokens
        )

        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(self.model_name)
            logger.info(f"GeminiService initialized: model='{self.model_name}', temperature={self.temperature}.")
        except Exception as e:
            logger.error(f"Error during GeminiService init (model: {self.model_name}): {e}", exc_info=True)
            raise RuntimeError(f"Failed to initialize Gemini Service (model: {self.model_name}): {e}")

    def generate_text(self, prompt: str) -> str:
        """
        Generates text using the configured Gemini model.

        Args:
            prompt (str): The prompt to send to the Gemini API.

        Returns:
            str: The text response generated by the Gemini API.

        Raises:
            RuntimeError: If there's an error during text generation.
        """
        if not prompt:
            logger.warning("Generate_text called with an empty prompt.")
            return ""

        try:
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config
            )
            # TODO: Add more sophisticated error handling for response.prompt_feedback
            return response.text
        except Exception as e:
            logger.error(f"Error during Gemini text generation: {e}", exc_info=True)
            raise RuntimeError(f"Failed to generate text using Gemini: {e}")

    def check_connection(self) -> bool:
        """
        Performs a lightweight check to see if the configured Gemini API key
        is valid and can connect to the Gemini service by listing available models.

        Returns:
            bool: True if the connection and a simple API call succeed, False otherwise.
        """
        try:
            # Listing models is a lightweight way to check API key validity and connectivity.
            models = genai.list_models()
            # Optionally, you could check if a specific model (e.g., 'gemini-pro') exists in the list
            # For now, just succeeding in listing models is sufficient.
            if models:
                 logger.info("Gemini API connection check successful: Models listed.")
                 return True
            else:
                 logger.warning("Gemini API connection check: list_models returned empty but no exception.")
                 return False # Or True, depending on how strictly you define "success"
        except Exception as e:
            logger.error(f"Gemini API connection check failed: {e}", exc_info=True)
            return False

# Example of how to use (not part of the class, for illustration):
# if __name__ == '__main__':
#     # This assumes CustomVariables.py is in a place where it can be imported
#     # and GEMINI_API_KEY is set as an environment variable.
#     import os
#     from src.e_Infra.CustomVariables import GEMINI_API_KEY # Adjust import path as needed
#
#     if not GEMINI_API_KEY:
#         print("Please set the GEMINI_API_KEY environment variable.")
#     else:
#         try:
#             gemini_service = GeminiService(api_key=GEMINI_API_KEY)
#             if gemini_service.check_connection():
#                 print("Gemini connection successful.")
#                 # Example text generation
#                 # text_prompt = "Tell me a fun fact about the Roman Empire."
#                 # generated_text = gemini_service.generate_text(text_prompt)
#                 # print(f"Prompt: {text_prompt}")
#                 # print(f"Generated Text: {generated_text}")
#             else:
#                 print("Gemini connection failed.")
#         except Exception as e:
#             print(f"An error occurred: {e}")
